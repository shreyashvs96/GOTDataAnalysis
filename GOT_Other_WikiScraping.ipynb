{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOT_Other_WikiScraping.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "lMCDpFcBh3hu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import lxml.html as lh\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "\n",
        "# Other characters\n",
        "url = \"https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_characters\"\n",
        "page = requests.get(url)\n",
        "doc = lh.fromstring(page.content)\n",
        "characters = doc.xpath('//div[@class = \"div-col columns column-width\"]//ul//li')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HpbX-kMbiYaH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create a dataframe to store scraped data about OTHER characters\n",
        "#Step 1 - Extarct list of characters from Wikipedia \n",
        "\n",
        "df_MSChars = pd.DataFrame(columns = ['Name', 'Google search query', 'GOT Fandom Wiki Link', 'Season(s)', 'First seen', 'FS_Episode_URL', 'FS_Episode_AirTime', 'Appeared in', 'Also known as', 'Titles', 'Origin', 'Status', 'Death', 'Death shown in','DS_Episode_URL', 'DS_Episode_AirTime', 'Allegiance', 'Culture', 'Religion', 'Portrayed by'], index = range(len(characters)))\n",
        "j = 0\n",
        "\n",
        "for i in range(len(characters)):\n",
        "  if characters[i].text_content().rsplit(\"as \", 1)[1] : \n",
        "    df_MSChars.loc[j]['Name'] = characters[i].text_content().rsplit(\"as \", 1)[1]\n",
        "    print(characters[i].text_content().rsplit(\"as \", 1)[1])\n",
        "    j+=1\n",
        "\n",
        "df_MSChars.dropna(how= 'all', inplace = True)    \n",
        "print(df_MSChars.head(5), df_MSChars.tail(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ljcfvaQqkdb9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Step 2 - Generate google search query URLs and extarct clean GOT Fandom Wiki links\n",
        "print(j)\n",
        "for i in range(j):\n",
        "  SearchQ = df_MSChars.loc[i]['Name']+ str(\" Game of thrones\") # Generate search string by removing space and \" characters from name and add GOT at the end of \n",
        "  googleSearch = \"https://www.google.com/search?q={}\".format(SearchQ)\n",
        "  \n",
        "  df_MSChars.loc[i]['Google search query'] = googleSearch\n",
        "  page = requests.get(googleSearch)\n",
        "  doc = lh.fromstring(page.content)\n",
        "  \n",
        "  wikilinks = doc.xpath('//a[contains(@href,\"https://gameofthrones.fandom.com/wiki/\")]/@href')\n",
        "  \n",
        "  #Clean page urls returned in google search result\n",
        "  import re   \n",
        "\n",
        "  to_remove = []\n",
        "  clean_links = []\n",
        "  for x, l in enumerate(wikilinks):\n",
        "    clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
        "\n",
        "    # Anything that doesn't fit the above pattern will be removed\n",
        "    if clean is None:\n",
        "        to_remove.append(x)\n",
        "        continue\n",
        "    clean_links.append(clean.group(1))\n",
        "   \n",
        "  if len(clean_links) : df_MSChars.loc[i]['GOT Fandom Wiki Link'] = clean_links[0] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c7l4JSx7kwSM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df_MSChars = df_MSChars.dropna(how = 'all') # Remove null valued rows scraped from Wikipedia page \n",
        "print(len(df_MSChars[df_MSChars['GOT Fandom Wiki Link'].isna()]))\n",
        "print(df_MSChars[df_MSChars['GOT Fandom Wiki Link'].isna()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ok4-GG0k-Wf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 3 - Scrape data from GOT Fandom Wiki pages\n",
        "j = 0\n",
        "\n",
        "for a in range(len(df_MSChars.index)) :\n",
        "  if df_MSChars.loc[a]['GOT Fandom Wiki Link'] :\n",
        "    page = requests.get(df_MSChars.loc[a]['GOT Fandom Wiki Link'])\n",
        "    doc = lh.fromstring(page.content)\n",
        "    data = doc.xpath('//div[@class = \"pi-item pi-data pi-item-spacing pi-border-color\"]')\n",
        "  \n",
        "    for i in range(len(data)):\n",
        "      if \"Season(s)\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Season(s)'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"First seen\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['First seen'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "  \n",
        "        #First seen episode crawling\n",
        "        FS_Episode_URL = doc.xpath('//div[@class = \"pi-item pi-data pi-item-spacing pi-border-color\" and contains(string(), \"First seen\")]//a/@href')\n",
        "        if len(DS_Episode_URL) :\n",
        "          df_MSChars.loc[j]['FS_Episode_URL'] = str(\"https://gameofthrones.fandom.com\") + FS_Episode_URL[0]\n",
        "  \n",
        "          fse = df_MSChars.loc[j]['FS_Episode_URL']\n",
        "          fse_page = requests.get(fse)\n",
        "          fse_doc = lh.fromstring(fse_page.content)\n",
        "          fse_x = fse_doc.xpath('//div[@class = \"pi-item pi-data pi-item-spacing pi-border-color\" and contains(string(), \"Air date\")]')\n",
        "      \n",
        "          if len(fse_x) :\n",
        "            df_MSChars.loc[j][\"FS_Episode_AirTime\"] = fse_x[0].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "  \n",
        "      if \"Appeared in\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Appeared in'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"Also known as\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Also known as'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"Titles\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Titles'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"Origin\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Origin'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\")\", \"), \").replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"Status\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Status'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"Death\" in data[i].text_content() and \"Death shown in\" not in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Death'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"Death shown in\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Death shown in'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "    \n",
        "        #Death shown episode crawling\n",
        "        DS_Episode_URL = doc.xpath('//div[@class = \"pi-item pi-data pi-item-spacing pi-border-color\" and contains(string(), \"Death shown in episode\")]//a/@href')\n",
        "        if len(DS_Episode_URL) : \n",
        "          df_MSChars.loc[j]['DS_Episode_URL'] = str(\"https://gameofthrones.fandom.com\") + DS_Episode_URL[0]\n",
        "  \n",
        "          dse = df_MSChars.loc[j]['DS_Episode_URL']\n",
        "          dse_page = requests.get(dse)\n",
        "          dse_doc = lh.fromstring(dse_page.content)\n",
        "          dse_x = dse_doc.xpath('//div[@class = \"pi-item pi-data pi-item-spacing pi-border-color\" and contains(string(), \"Air date\")]')\n",
        "      \n",
        "          if len(dse_x) :\n",
        "            df_MSChars.loc[j][\"DS_Episode_AirTime\"] = dse_x[0].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "  \n",
        "      if \"Allegiance\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Allegiance'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\")\", \"), \").replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"Culture\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Culture'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"Religion\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Religion'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\"\\n\", \"\")\n",
        "    \n",
        "      if \"Portrayed by\" in data[i].text_content() :\n",
        "        df_MSChars.loc[j]['Portrayed by'] = data[i].text_content().rsplit('\\t', 1)[1].replace(\")\", \"), \").replace(\"\\n\", \"\")\n",
        "   \n",
        "    j+=1  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XREZIJcosJk0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(df_MSChars.head(5))\n",
        "#Copy the df into a CSV file and export it on the device\n",
        "df_MSChars.to_csv('GOT_Others.csv', index = False)\n",
        "files.download('GOT_Others.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}